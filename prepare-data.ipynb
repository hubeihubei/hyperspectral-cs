{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare csvs for autoencoder data loader\n",
    "\n",
    "Creates a csv of a dataset for hyperspectral images to be fed into the autoencoder. Each row contains the following information:\n",
    "\n",
    "1. type of data (exr or png)\n",
    "2. path to directory containing the data\n",
    "3. root name of the data file (if png) or the name of the exr file if (exr)\n",
    "4. Row of upper left corner of patch\n",
    "5. Col of upper left corner of patch\n",
    "6. Final side length of patch (should be square)\n",
    "7. scale factor (1, 0.5, or 2, according to whether the patch is to be used as-is, downsampled by 0.5, or upscaled by 2)\n",
    "8. normal/flipped - whether or not the image is to be flipped across the vertical axis or not.\n",
    "\n",
    "See implementation of HyperspectralDataset for the way to use the resulting .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting into training, val, and test datasets\n",
    "def split_train_val_test(ntotal, train, val):\n",
    "    \"\"\"total: int - number of examples\n",
    "    train: float in [0, 1] - fraction of total that are training\n",
    "    val: float in [0, 1-train] - fraction of remaining that are val\n",
    "    \"\"\"\n",
    "    x = np.random.permutation(range(ntotal))\n",
    "    traincutoff = int(ntotal*train)\n",
    "    valcutoff = int(ntotal*(train+val))\n",
    "    # Train, Val, Test\n",
    "    return x[:traincutoff].tolist(), x[traincutoff:valcutoff].tolist(), x[valcutoff:].tolist()\n",
    "\n",
    "# Function for writing a subset of a dataset to a file\n",
    "def write_dataset(alldata, fieldnames, selection, filepath):\n",
    "    with open(filepath, \"w\") as f:     \n",
    "        writer = csv.DictWriter(f, fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i in selection:\n",
    "            writer.writerow(alldata[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAIST\n",
    "import OpenEXR as exr\n",
    "KAISTdir = \"data/KAIST\"\n",
    "KAISTimages = [\"scene{:02}_reflectance.exr\".format(i) for i in range(1,31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAVE\n",
    "from PIL import Image\n",
    "CAVEdir = \"data/CAVE\"\n",
    "g = os.walk(CAVEdir)\n",
    "_, CAVEimages, _ = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(samples_per_image, imagetype, imagedir, imagename, side_length, nrows, ncols):\n",
    "    data = []\n",
    "    for scale in [1, 0.5, 2]:\n",
    "        for flip in [False, True]:\n",
    "            for _ in range(samples_per_image):\n",
    "                sample = {}\n",
    "                sample[\"type\"] = imagetype\n",
    "                sample[\"dir\"] = imagedir\n",
    "                sample[\"name\"] = imagename\n",
    "                sample[\"side\"] = side_length\n",
    "                # Sample a random coordinate for the top left corner.\n",
    "                # Adjust appropriately according to the scale factor\n",
    "                row = np.random.randint(0, nrows-(side/scale))\n",
    "                col = np.random.randint(0, ncols-(side/scale))\n",
    "                sample[\"row\"] = row\n",
    "                sample[\"col\"] = col\n",
    "                sample[\"scale\"] = scale\n",
    "                sample[\"flip\"] = flip\n",
    "                data.append(sample)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling parameters\n",
    "side = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # Controls data split\n",
    "KAISTdata = []\n",
    "samples_per_image = 70\n",
    "fieldnames = [\"type\", \"dir\", \"name\", \"row\", \"col\", \"side\", \"scale\", \"flip\"]\n",
    "for image in KAISTimages:\n",
    "    # Load image and extract dimensions:\n",
    "    file = exr.InputFile(os.path.join(KAISTdir, image))\n",
    "    header = file.header()\n",
    "    ncols = header[\"displayWindow\"].max.x+1\n",
    "    nrows = header[\"displayWindow\"].max.y+1\n",
    "    file.close()\n",
    "    KAISTdata += generate_samples(samples_per_image, \"exr\", KAISTdir, image, side, nrows, ncols)\n",
    "                \n",
    "train, val, test = split_train_val_test(len(KAISTdata), 0.8, 0.1)\n",
    "\n",
    "# Train\n",
    "write_dataset(KAISTdata, fieldnames, train, \"data/kaist_set/kaist_train.csv\")\n",
    "# Val\n",
    "write_dataset(KAISTdata, fieldnames, val, \"data/kaist_set/kaist_val.csv\")\n",
    "# Test\n",
    "write_dataset(KAISTdata, fieldnames, test, \"data/kaist_set/kaist_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAVEdata = []\n",
    "samples_per_image = 30\n",
    "fieldnames = [\"type\", \"dir\", \"name\", \"row\", \"col\", \"side\", \"scale\", \"flip\"]\n",
    "for image in CAVEimages:\n",
    "    # Load image and extract dimensions:\n",
    "    imagedir = os.path.join(CAVEdir, image, image)\n",
    "    imagefile = \"{}_01.png\".format(image)\n",
    "    img = Image.open(os.path.join(imagedir, imagefile))\n",
    "    nrows, ncols = img.size\n",
    "    \n",
    "    CAVEdata += generate_samples(samples_per_image, \"png\", imagedir, image, side, nrows, ncols)\n",
    "               \n",
    "with open(\"data/cave_data.csv\", \"w\") as f:     \n",
    "    writer = csv.DictWriter(f, fieldnames)\n",
    "    writer.writeheader()\n",
    "    for entry in CAVEdata:\n",
    "        writer.writerow(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = KAISTdata + CAVEdata\n",
    "with open(\"data/combined_data.csv\", \"w\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames)\n",
    "    writer.writeheader()\n",
    "    for entry in alldata:\n",
    "        writer.writerow(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'channels': {'B': HALF (1, 1),\n",
       "  'G': HALF (1, 1),\n",
       "  'R': HALF (1, 1),\n",
       "  'w420nm': HALF (1, 1),\n",
       "  'w430nm': HALF (1, 1),\n",
       "  'w440nm': HALF (1, 1),\n",
       "  'w450nm': HALF (1, 1),\n",
       "  'w460nm': HALF (1, 1),\n",
       "  'w470nm': HALF (1, 1),\n",
       "  'w480nm': HALF (1, 1),\n",
       "  'w490nm': HALF (1, 1),\n",
       "  'w500nm': HALF (1, 1),\n",
       "  'w510nm': HALF (1, 1),\n",
       "  'w520nm': HALF (1, 1),\n",
       "  'w530nm': HALF (1, 1),\n",
       "  'w540nm': HALF (1, 1),\n",
       "  'w550nm': HALF (1, 1),\n",
       "  'w560nm': HALF (1, 1),\n",
       "  'w570nm': HALF (1, 1),\n",
       "  'w580nm': HALF (1, 1),\n",
       "  'w590nm': HALF (1, 1),\n",
       "  'w600nm': HALF (1, 1),\n",
       "  'w610nm': HALF (1, 1),\n",
       "  'w620nm': HALF (1, 1),\n",
       "  'w630nm': HALF (1, 1),\n",
       "  'w640nm': HALF (1, 1),\n",
       "  'w650nm': HALF (1, 1),\n",
       "  'w660nm': HALF (1, 1),\n",
       "  'w670nm': HALF (1, 1),\n",
       "  'w680nm': HALF (1, 1),\n",
       "  'w690nm': HALF (1, 1),\n",
       "  'w700nm': HALF (1, 1),\n",
       "  'w710nm': HALF (1, 1),\n",
       "  'w720nm': HALF (1, 1)},\n",
       " 'compression': ZIP_COMPRESSION,\n",
       " 'dataWindow': (0, 0) - (3375, 2703),\n",
       " 'displayWindow': (0, 0) - (3375, 2703),\n",
       " 'lineOrder': INCREASING_Y,\n",
       " 'pixelAspectRatio': 1.0,\n",
       " 'screenWindowCenter': (0.0, 0.0),\n",
       " 'screenWindowWidth': 1.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = exr.InputFile(\"data/KAIST/scene01_reflectance.exr\")\n",
    "file.header()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "bool(torch.FloatTensor([float('inf')]).numpy() > torch.FloatTensor([float(2)]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare csvs for autoencoder data loader\n",
    "\n",
    "Creates a csv of a dataset for hyperspectral images to be fed into the autoencoder. Each row contains the following information:\n",
    "\n",
    "1. type of data (exr or png)\n",
    "2. path to directory containing the data\n",
    "3. root name of the data file (if png) or the name of the exr file if (exr)\n",
    "4. Row of upper left corner of patch\n",
    "5. Col of upper left corner of patch\n",
    "6. Final side length of patch (should be square)\n",
    "7. scale factor (1, 0.5, or 2, according to whether the patch is to be used as-is, downsampled by 0.5, or upscaled by 2)\n",
    "8. normal/flipped - whether or not the image is to be flipped across the vertical axis or not.\n",
    "\n",
    "See implementation of HyperspectralDataset for the way to use the resulting .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAIST\n",
    "import OpenEXR as exr\n",
    "KAISTdir = \"data/KAIST\"\n",
    "KAISTimages = [\"scene{:02}_reflectance.exr\".format(i) for i in range(1,31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAVE\n",
    "from PIL import Image\n",
    "CAVEdir = \"data/CAVE\"\n",
    "g = os.walk(CAVEdir)\n",
    "_, CAVEimages, _ = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_samples(samples_per_image, imagetype, imagedir, imagename, side_length, nrows, ncols):\n",
    "    data = []\n",
    "    for scale in [1, 0.5, 2]:\n",
    "        for flip in [False, True]:\n",
    "            for _ in range(samples_per_image):\n",
    "                sample = {}\n",
    "                sample[\"type\"] = imagetype\n",
    "                sample[\"dir\"] = imagedir\n",
    "                sample[\"name\"] = imagename\n",
    "                sample[\"side\"] = side_length\n",
    "                # Sample a random coordinate for the top left corner.\n",
    "                # Adjust appropriately according to the scale factor\n",
    "                row = np.random.randint(0, nrows-(side/scale))\n",
    "                col = np.random.randint(0, ncols-(side/scale))\n",
    "                sample[\"row\"] = row\n",
    "                sample[\"col\"] = col\n",
    "                sample[\"scale\"] = scale\n",
    "                sample[\"flip\"] = flip\n",
    "                data.append(sample)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting into training, val, and test datasets\n",
    "def split_train_val_test(ntotal, train, val):\n",
    "    \"\"\"total: int - number of examples\n",
    "    train: float in [0, 1] - fraction of total that are training\n",
    "    val: float in [0, 1-train] - fraction of remaining that are val\n",
    "    \"\"\"\n",
    "    x = np.random.permutation(range(ntotal))\n",
    "    traincutoff = int(ntotal*train)\n",
    "    valcutoff = int(ntotal*(train+val))\n",
    "    # Train, Val, Test\n",
    "    return x[:traincutoff].tolist(), x[traincutoff:valcutoff].tolist(), x[valcutoff:].tolist()\n",
    "\n",
    "# Function for writing a subset of a dataset to a file\n",
    "def write_dataset(alldata, fieldnames, selection, filepath):\n",
    "    \"\"\"alldata is a list of lists, where each sublist is a list of \n",
    "    entries to be written to the csv file.\"\"\"\n",
    "    with open(filepath, \"w\") as f:     \n",
    "        writer = csv.DictWriter(f, fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i in selection:\n",
    "            for entry in alldata[i]:\n",
    "                writer.writerow(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling parameters\n",
    "side = 96\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write KAIST data only\n",
    "np.random.seed(0) # Controls data split\n",
    "KAISTdata = [] # List of lists\n",
    "samples_per_image = 120\n",
    "fieldnames = [\"type\", \"dir\", \"name\", \"row\", \"col\", \"side\", \"scale\", \"flip\"]\n",
    "for image in KAISTimages:\n",
    "    # Load image and extract dimensions:\n",
    "    file = exr.InputFile(os.path.join(KAISTdir, image))\n",
    "    header = file.header()\n",
    "    ncols = header[\"displayWindow\"].max.x+1\n",
    "    nrows = header[\"displayWindow\"].max.y+1\n",
    "    file.close()\n",
    "    KAISTdata.append(generate_samples(samples_per_image, \"exr\", KAISTdir, image, side, nrows, ncols))\n",
    "                \n",
    "# train, val, test = split_train_val_test(len(KAISTdata), 0.8, 0.1)\n",
    "\n",
    "# # Train\n",
    "# write_dataset(KAISTdata, fieldnames, train, \"data/kaist_set/kaist_train_large.csv\")\n",
    "# # Val\n",
    "# write_dataset(KAISTdata, fieldnames, val, \"data/kaist_set/kaist_val_large.csv\")\n",
    "# # Test\n",
    "# write_dataset(KAISTdata, fieldnames, test, \"data/kaist_set/kaist_test_large.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAVEdata = []\n",
    "# Images with weird behavior:\n",
    "blacklist = [\"watercolors_ms\"]\n",
    "samples_per_image = 30\n",
    "fieldnames = [\"type\", \"dir\", \"name\", \"row\", \"col\", \"side\", \"scale\", \"flip\"]\n",
    "for image in CAVEimages:\n",
    "    if image in blacklist:\n",
    "        continue\n",
    "    # Load image and extract dimensions:\n",
    "    imagedir = os.path.join(CAVEdir, image, image)\n",
    "    imagefile = \"{}_01.png\".format(image)\n",
    "    img = Image.open(os.path.join(imagedir, imagefile))\n",
    "    nrows, ncols = img.size\n",
    "    CAVEdata.append(generate_random_samples(samples_per_image, \"png\", imagedir, image, side, nrows, ncols))\n",
    "               \n",
    "# with open(\"data/cave_data.csv\", \"w\") as f:     \n",
    "#     writer = csv.DictWriter(f, fieldnames)\n",
    "#     writer.writeheader()\n",
    "#     for entry in CAVEdata:\n",
    "#         writer.writerow(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = KAISTdata + CAVEdata\n",
    "\n",
    "train, val, test = split_train_val_test(len(alldata), 0.8, 0.1)\n",
    "\n",
    "# Train\n",
    "write_dataset(alldata, fieldnames, train, \"data/train_large.csv\")\n",
    "# Val\n",
    "write_dataset(alldata, fieldnames, val, \"data/val_large.csv\")\n",
    "# Test\n",
    "write_dataset(alldata, fieldnames, test, \"data/test_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

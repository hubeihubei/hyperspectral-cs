{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda as cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# Cuda active?\n",
    "if cuda.is_available():\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    print(\"GPU: {}\".format(cuda.get_device_name(0)))\n",
    "    cuda_available = True\n",
    "else:\n",
    "    cuda_available = False\n",
    "    print(\"Cuda unavailable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile hyperspectral_dataset.py\n",
    "# Load data\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import csv, numpy as np\n",
    "import os\n",
    "import OpenEXR as exr, Imath\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "####################\n",
    "# Helper functions #\n",
    "####################\n",
    "\n",
    "def readEXRImage(filepath, channelrange):\n",
    "    \"\"\"Helper function for reading .exr files from the KAIST dataset.\n",
    "    Returns an array with dimension ordering (C, H, W) as required by pytorch.\n",
    "    \"\"\"\n",
    "    file = exr.InputFile(filepath)\n",
    "    channels = [\"w{}nm\".format(wavelength) for wavelength in channelrange]\n",
    "    header = file.header()\n",
    "    ncols = header[\"displayWindow\"].max.x+1\n",
    "    nrows = header[\"displayWindow\"].max.y+1\n",
    "    pt = Imath.PixelType(Imath.PixelType.HALF)\n",
    "    imgstrs = file.channels(channels, pt)\n",
    "    full = np.zeros((len(channels), nrows, ncols), dtype=np.float16)\n",
    "    for i, imgstr in enumerate(imgstrs):\n",
    "        red = np.frombuffer(imgstr, dtype=np.float16)\n",
    "        full[i,:,:] = np.reshape(red, (nrows, ncols))\n",
    "    return full\n",
    "\n",
    "def readPNGImage(filedir, filename, channelrange, minwavelength, increment):\n",
    "    \"\"\"Helper function for reading patches from the set of png files from the CAVE dataset.\n",
    "    \"\"\"\n",
    "    indexes = [int((wavelength-minwavelength)/increment + 1) for wavelength in channelrange]\n",
    "    # Get image size:\n",
    "    img = Image.open(os.path.join(filedir, \"{}_{:02}.png\".format(filename, 1)))\n",
    "    full = np.zeros((len(channelrange),) + img.size, dtype=np.float16)\n",
    "    img.close()\n",
    "    for i, index in enumerate(indexes):\n",
    "        img = Image.open(os.path.join(filedir, \"{}_{:02}.png\".format(filename, index)))\n",
    "        # Divide by max val of np.uint16 to normalize image\n",
    "        full[i,:,:] = np.array(img, dtype=np.float32)/np.iinfo(np.uint16).max\n",
    "    return full\n",
    "\n",
    "###########\n",
    "# Dataset #\n",
    "###########\n",
    "# For reference, the csv field names: \n",
    "# fieldnames = [\"type\", \"dir\", \"name\", \"row\", \"col\", \"side\", \"scale\", \"flip\"]\n",
    "\n",
    "class HyperspectralDataset(Dataset):\n",
    "    def __init__(self, csvfile, minwavelength, maxwavelength, nchannels, height, width):\n",
    "        \"\"\"Open and load the lines of the csvfile.\"\"\"\n",
    "        self.minwavelength = minwavelength\n",
    "        self.maxwavelength = maxwavelength\n",
    "        self.channelrange = range(self.minwavelength, self.maxwavelength+1, 10)\n",
    "        \n",
    "        \n",
    "        # Store dictionaries mapping the image name to\n",
    "        # - a list of entries that stem from that image\n",
    "        # - the datatype (exr or png) of the image\n",
    "        # - the filepath to that image\n",
    "        self.update_entries(csvfile)\n",
    "\n",
    "    def update_entries(self, csvfile):\n",
    "        \"\"\"Refresh the dataset without reloading the images, but adding a different set of csv files.\"\"\"\n",
    "        self.dataEntries = []\n",
    "        self.namesToImages = {}\n",
    "        with open(csvfile, \"r\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for entry in reader:\n",
    "                self.dataEntries.append(entry)\n",
    "                # Add the full image to the dict if not already included.\n",
    "                if entry[\"name\"] not in self.namesToImages:\n",
    "                    print(\"loading {}\".format(entry[\"name\"]))\n",
    "                    # Need to read the image into the dictionary\n",
    "                    if entry[\"type\"] == \"exr\":\n",
    "                        filepath = os.path.join(entry[\"dir\"], entry[\"name\"])\n",
    "                        self.namesToImages[entry[\"name\"]] = readEXRImage(filepath, self.channelrange)\n",
    "                    elif entry[\"type\"] == \"png\":\n",
    "                        self.namesToImages[entry[\"name\"]] = readPNGImage(entry[\"dir\"], entry[\"name\"],\n",
    "                                                                           self.channelrange,\n",
    "                                                                           self.minwavelength,\n",
    "                                                                           self.maxwavelength)\n",
    "                    else:\n",
    "                        raise ValueError(\"Invalid entry at row {} - Cannot load data of type '{}'.\".format(idx, dtype))\n",
    "                    print(\"\\tSize: {} bytes.\".format(self.namesToImages[entry[\"name\"]].nbytes))\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.dataEntries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.dataEntries[idx]\n",
    "        image = self.namesToImages[entry[\"name\"]]\n",
    "        side = int(entry[\"side\"])\n",
    "        scale = float(entry[\"scale\"])\n",
    "        i = int(entry[\"row\"])\n",
    "        j = int(entry[\"col\"])\n",
    "        shape = (int(side/scale),\n",
    "                 int(side/scale)\n",
    "                )\n",
    "        patch = image[:, i:i+shape[0], j:j+shape[0]]\n",
    "        # Resizing\n",
    "        patch = resize(patch, (patch.shape[0], side, side), mode=\"constant\")\n",
    "        # Flip if necessary:\n",
    "        if entry[\"flip\"]:\n",
    "            patch = np.flip(patch, axis=2) # Horizontal flip\n",
    "\n",
    "        # Convert to Torch tensor and return\n",
    "        out = torch.Tensor(patch.copy()).type(dtype).cpu()\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile build_autoencoder.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "from collections import OrderedDict\n",
    "\n",
    "def build_autoencoder(height, width, inchannels, outchannels, filtersize, nlayers):\n",
    "    samepad = int((filtersize-1)/2)\n",
    "\n",
    "    ###########\n",
    "    # Encoder #\n",
    "    ###########\n",
    "    encoderLayers = [nn.Conv2d(inchannels, outchannels, filtersize, padding=samepad)]\n",
    "    for i in range(nlayers-1):\n",
    "        encoderLayers += [nn.ReLU(), nn.Conv2d(outchannels, outchannels, filtersize, padding=samepad)]\n",
    "    Encoder = nn.Sequential(*encoderLayers)\n",
    "\n",
    "    ###########\n",
    "    # Decoder #\n",
    "    ########### \n",
    "    decoderLayers = [] \n",
    "    for i in range(nlayers-1):\n",
    "        decoderLayers += [nn.ReLU(), nn.Conv2d(outchannels, outchannels, filtersize, padding=samepad)]\n",
    "    decoderLayers += [nn.ReLU(), nn.Conv2d(outchannels, inchannels, filtersize, padding=samepad), nn.ReLU()]\n",
    "    Decoder = nn.Sequential(*decoderLayers)\n",
    "\n",
    "    ###############\n",
    "    # Autoencoder #\n",
    "    ###############\n",
    "    Autoencoder = nn.Sequential(OrderedDict([\n",
    "        (\"Encoder\", Encoder),\n",
    "        (\"Decoder\", Decoder)])\n",
    "    )\n",
    "    return Autoencoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile utils.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.cuda as cuda\n",
    "import matplotlib.pyplot as plt\n",
    "# Helper functions\n",
    "\n",
    "#################\n",
    "# Checkpointing #\n",
    "#################\n",
    "def save_checkpoint(state, is_best, filename='/output/checkpoint.pth.tar', always_save=False):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best or always_save:\n",
    "        print (\"=> Saving checkpoint to: {}\".format(filename))\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")\n",
    "\n",
    "####################\n",
    "# Loss Computation #\n",
    "####################\n",
    "def compute_loss(loss, output, data, lam, model):\n",
    "    out = 0.5*loss(output, data)\n",
    "    # Extract layer weights for regularization\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            out += lam*(param.norm()**2)\n",
    "    return out\n",
    "\n",
    "##############\n",
    "# Validation #\n",
    "##############\n",
    "def validate(loss, model, val_data, lam, weights, cuda_available):\n",
    "    \"\"\"Computes the validation error of the model on the validation set.\n",
    "    val_data should be a Dataset where the first batch loads all of the data.\"\"\"\n",
    "    val_loader = DataLoader(val_data, batch_size=4, shuffle=True)\n",
    "    _, val_tensor = next(enumerate(val_loader))\n",
    "    if cuda.is_available():\n",
    "        val_tensor.cuda()\n",
    "    val_set = Variable(val_tensor, requires_grad = False)\n",
    "    output = model(val_set)\n",
    "    return compute_loss(loss, output, val_set, lam, weights)\n",
    "\n",
    "############\n",
    "# Plotting #\n",
    "############\n",
    "def save_train_val_loss_plots(trainlosses, vallosses, epoch):\n",
    "    # Train loss\n",
    "    fig = plt.figure()\n",
    "    plt.plot(trainlosses)\n",
    "    plt.title(\"Train loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"trainloss_epoch{}.png\".format(epoch))\n",
    "    # Train loss\n",
    "    fig = plt.figure()\n",
    "    plt.plot(trainlosses)\n",
    "    plt.title(\"Val loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"Val loss{}.png\".format(epoch))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointfile: None\n",
      "start_epoch: 0\n",
      "optimizer: <torch.optim.adam.Adam object at 0x7eff62d6cc88>\n",
      "batch_size: 64\n",
      "num_epochs: 60\n",
      "learning rate (initial): 1e-05\n"
     ]
    }
   ],
   "source": [
    "# Set up training.\n",
    "checkpointfile = None\n",
    "lam = 1e-8 # Weight decay parameter for L2 regularization\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 60\n",
    "batch_size = 64\n",
    "\n",
    "# Build model and loss\n",
    "# Hyperparameters\n",
    "height = 96\n",
    "width = 96\n",
    "inchannels = 29\n",
    "outchannels = 64\n",
    "filtersize = 3\n",
    "nlayers = 11\n",
    "samepad = int((filtersize-1)/2)\n",
    "\n",
    "Autoencoder = build_autoencoder(height, width, inchannels, outchannels, filtersize, nlayers)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "if cuda.is_available():\n",
    "    Autoencoder.cuda()\n",
    "    loss.cuda()\n",
    "\n",
    "# Checkpointing\n",
    "if checkpointfile is not None:\n",
    "    if cuda_available:\n",
    "        checkpoint = torch.load(checkpointfile)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(checkpointfile,\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    Autoencoder.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer = optim.Adam(Autoencoder.parameters(), lr=learning_rate)\n",
    "    optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "    trainlosses = checkpoint['trainlosses']\n",
    "#     vallosses = checkpoint['vallosses']\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_loss = torch.FloatTensor([float('inf')])\n",
    "    optimizer = optim.Adam(Autoencoder.parameters(), lr=learning_rate)\n",
    "    trainlosses = []\n",
    "    vallosses = []\n",
    "    # Initialize weights:\n",
    "    for name, param in Autoencoder.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            nn.init.xavier_uniform(param)\n",
    "        elif \"bias\" in name:\n",
    "            nn.init.constant(param, 0)\n",
    "            \n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30], gamma=0.1)\n",
    "scheduler.last_epoch = start_epoch - 1\n",
    "\n",
    "# Print summary of setup:\n",
    "print(\"checkpointfile: {}\".format(checkpointfile))\n",
    "print(\"start_epoch: {}\".format(start_epoch))\n",
    "print(\"optimizer: {}\".format(optimizer))\n",
    "print(\"batch_size: {}\".format(batch_size))\n",
    "print(\"num_epochs: {}\".format(num_epochs))\n",
    "print(\"learning rate (initial): {}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scene14_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene23_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading real_and_fake_apples_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene26_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene21_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene18_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading fake_and_real_lemons_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene30_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading fake_and_real_beers_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading superballs_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene09_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene24_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene15_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene27_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading thread_spools_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading fake_and_real_food_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene29_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading chart_and_stuffed_toy_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading fake_and_real_strawberries_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading photo_and_face_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene07_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene02_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene01_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading pompoms_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading face_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene19_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading hairs_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene11_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading feathers_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene13_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene05_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene04_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene08_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading fake_and_real_lemon_slices_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene25_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading scene03_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading real_and_fake_peppers_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene22_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading beads_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading jelly_beans_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading glass_tiles_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading fake_and_real_peppers_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading balloons_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene16_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading cd_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading oil_painting_ms\n",
      "\tSize: 15204352 bytes.\n",
      "loading scene06_reflectance.exr\n",
      "\tSize: 529464832 bytes.\n",
      "loading fake_and_real_sushi_ms\n",
      "\tSize: 15204352 bytes.\n",
      "Images in training data set: 22140\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Load Training Data #\n",
    "######################\n",
    "# Note: only need to do once.\n",
    "rebuild_dataset = True\n",
    "train_data_csv = \"data/train_large.csv\"\n",
    "if rebuild_dataset:\n",
    "    train_data = HyperspectralDataset(train_data_csv, 420, 700, inchannels, height, width)\n",
    "else:\n",
    "    train_data.update_entries(train_data_csv)\n",
    "\n",
    "print(\"Images in training data set: {}\". format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "\titeration: 0\ttrain loss: 0.00654228962957859\n",
      "\titeration: 10\ttrain loss: 0.006763617508113384\n",
      "\titeration: 20\ttrain loss: 0.00852988287806511\n",
      "\titeration: 30\ttrain loss: 0.00872713141143322\n",
      "\titeration: 40\ttrain loss: 0.00595971941947937\n",
      "\titeration: 50\ttrain loss: 0.012237140908837318\n",
      "\titeration: 60\ttrain loss: 0.008441410958766937\n",
      "\titeration: 70\ttrain loss: 0.009560192935168743\n",
      "\titeration: 80\ttrain loss: 0.006406854838132858\n",
      "\titeration: 90\ttrain loss: 0.0045232404954731464\n",
      "\titeration: 100\ttrain loss: 0.0030915180686861277\n",
      "\titeration: 110\ttrain loss: 0.004827448166906834\n",
      "\titeration: 120\ttrain loss: 0.0013804048066958785\n",
      "\titeration: 130\ttrain loss: 0.001844298210926354\n",
      "\titeration: 140\ttrain loss: 0.0018614917062222958\n",
      "\titeration: 150\ttrain loss: 0.001998655265197158\n",
      "\titeration: 160\ttrain loss: 0.0030847324524074793\n",
      "\titeration: 170\ttrain loss: 0.0021100991871207952\n",
      "\titeration: 180\ttrain loss: 0.005381548311561346\n",
      "\titeration: 190\ttrain loss: 0.0030192898120731115\n",
      "\titeration: 200\ttrain loss: 0.0020406062249094248\n",
      "\titeration: 210\ttrain loss: 0.0025403271429240704\n",
      "\titeration: 220\ttrain loss: 0.0029123579151928425\n",
      "\titeration: 230\ttrain loss: 0.0018140547908842564\n",
      "\titeration: 240\ttrain loss: 0.0015188709367066622\n",
      "\titeration: 250\ttrain loss: 0.0018531413516029716\n",
      "\titeration: 260\ttrain loss: 0.0017608177149668336\n",
      "\titeration: 270\ttrain loss: 0.0014419031795114279\n",
      "\titeration: 280\ttrain loss: 0.0020965125877410173\n",
      "\titeration: 290\ttrain loss: 0.0015163179486989975\n",
      "\titeration: 300\ttrain loss: 0.0011926805600523949\n",
      "\titeration: 310\ttrain loss: 0.0013517031911760569\n",
      "\titeration: 320\ttrain loss: 0.0015464933821931481\n",
      "\titeration: 330\ttrain loss: 0.002152752596884966\n",
      "\titeration: 340\ttrain loss: 0.001402683206833899\n",
      "=> Saving checkpoint to: checkpoints/checkpoint_epoch_0.pth.tar\n",
      "epoch: 1\n",
      "\titeration: 0\ttrain loss: 0.0017211134545505047\n",
      "\titeration: 10\ttrain loss: 0.0016582667594775558\n",
      "\titeration: 20\ttrain loss: 0.0012101746397092938\n",
      "\titeration: 30\ttrain loss: 0.001115461578592658\n",
      "\titeration: 40\ttrain loss: 0.0015116090653464198\n",
      "\titeration: 50\ttrain loss: 0.0013788231881335378\n",
      "\titeration: 60\ttrain loss: 0.00200195936486125\n",
      "\titeration: 70\ttrain loss: 0.002012788550928235\n",
      "\titeration: 80\ttrain loss: 0.0012813599314540625\n",
      "\titeration: 90\ttrain loss: 0.0007245097076520324\n",
      "\titeration: 100\ttrain loss: 0.001263102050870657\n",
      "\titeration: 110\ttrain loss: 0.0011239147279411554\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Run the training #\n",
    "####################\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    scheduler.step()\n",
    "    for it, input_ in enumerate(train_loader):\n",
    "        input_ = Variable(input_, requires_grad=False)\n",
    "        if cuda_available:\n",
    "            input_ = input_.cuda()\n",
    "        # New batch\n",
    "        scheduler.optimizer.zero_grad()\n",
    "        output = Autoencoder(input_)\n",
    "        trainloss = compute_loss(loss, output, input_, lam, Autoencoder)\n",
    "        trainloss.backward()\n",
    "        scheduler.optimizer.step()\n",
    "        if not (it % 10):\n",
    "            print(\"\\titeration: {}\\ttrain loss: {}\".format(it, trainloss.data[0]))\n",
    "        trainlosses.append(trainloss.data[0])\n",
    "    # Checkpointing\n",
    "    # Get bool not ByteTensor\n",
    "#     valloss = validate(loss, Autoencoder, val_data, lam, weights, cuda_available)\n",
    "#     print(\"End epoch {}\\tval loss: {}\".format(epoch, valloss.data[0]))\n",
    "#     vallosses.append(valloss.data[0])\n",
    "\n",
    "    is_best = bool(trainloss.data.cpu().numpy() < best_loss.numpy())\n",
    "    # Get greater Tensor to keep track best acc\n",
    "    best_loss = torch.FloatTensor(min(trainloss.data.cpu().numpy(), best_loss.numpy()))\n",
    "    # Save checkpoint\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': Autoencoder.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'optim_state_dict': optimizer.state_dict(),\n",
    "        'trainlosses': trainlosses\n",
    "#         'vallosses': vallosses\n",
    "    }, is_best, filename=\"checkpoints/checkpoint_epoch_{}.pth.tar\".format(epoch), always_save=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load val image(s)\n",
    "# val_data_csv =\"data/kaist_set/kaist_val_very_small.csv\"\n",
    "val_data_csv = \"data/kaist_set/kaist_val_very_small.csv\"\n",
    "val_data = HyperspectralDataset(val_data_csv, 420, 700, inchannels, height, width)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation losses\n",
    "# checkpointfile = \"overfit_checkpoints/checkpoint_epoch_499.pth.tar\"\n",
    "num_epochs = 60\n",
    "lam = 1e-8 # Weight decay parameter for L2 regularization\n",
    "_, input_ = next(enumerate(val_loader))\n",
    "if cuda_available:\n",
    "    input_ = input_.cuda()\n",
    "####################\n",
    "# Make Autoencoder #\n",
    "####################\n",
    "# Hyperparameters\n",
    "height = 96\n",
    "width = 96\n",
    "inchannels = 29\n",
    "outchannels = 64\n",
    "filtersize = 3\n",
    "nlayers = 11\n",
    "samepad = int((filtersize-1)/2)\n",
    "Autoencoder = build_autoencoder(height, width, inchannels, outchannels, filtersize, nlayers)\n",
    "if cuda.is_available():\n",
    "    Autoencoder.cuda()\n",
    "# Set to inference mode\n",
    "for param in Autoencoder[0].parameters():\n",
    "    param.requires_grad = False\n",
    "for param in Autoencoder[1].parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "input_ = Variable(input_, requires_grad=False)\n",
    "vallosses = []\n",
    "# Autoencoder = build_autoencoder(height, width, inchannels, outchannels, filtersize, nlayers)\n",
    "for epoch in range(num_epochs):\n",
    "#     Autoencoder = None\n",
    "#     torch.cuda.empty_cache()\n",
    "    checkpointfile = \"checkpoints/checkpoint_epoch_{}.pth.tar\".format(epoch)\n",
    "#     if cuda_available:\n",
    "    if cuda.is_available():\n",
    "        checkpoint = torch.load(checkpointfile)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(checkpointfile,\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage)\n",
    "#     Autoencoder = build_autoencoder(height, width, inchannels, outchannels, filtersize, nlayers).cuda()\n",
    "\n",
    "\n",
    "    Autoencoder.load_state_dict(checkpoint['state_dict'])\n",
    "    trainlosses = checkpoint[\"trainlosses\"]\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(checkpointfile, checkpoint['epoch']))\n",
    "\n",
    "\n",
    "\n",
    "    output = Autoencoder(input_)\n",
    "    valloss = compute_loss(loss, output, input_, lam, Autoencoder)\n",
    "#     del output\n",
    "#     del Autoencoder\n",
    "#     torch.cuda.empty_cache()\n",
    "    vallosses.append(valloss.data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val loss\n",
    "fig = plt.figure()\n",
    "plt.semilogy(vallosses)\n",
    "plt.title(\"Val loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.savefig(\"trainloss_epoch{}.png\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss\n",
    "fig = plt.figure()\n",
    "plt.semilogy(trainlosses)\n",
    "plt.title(\"Train loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.savefig(\"trainloss_epoch{}.png\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
